{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_36216\\1806910230.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\visha\\\\Downloads\\\\reviews_data_dump\\\\reviews_badminton\\\\data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "            Reviewer Name               Review Title  \\\n",
      "0            Kamal Suresh               Nice product   \n",
      "1       Flipkart Customer     Don't waste your money   \n",
      "2  A. S. Raja Srinivasan   Did not meet expectations   \n",
      "3     Suresh Narayanasamy                       Fair   \n",
      "4               ASHIK P A                Over priced   \n",
      "\n",
      "               Place of Review  Up Votes  Down Votes     Month  \\\n",
      "0   Certified Buyer, Chirakkal     889.0        64.0  Feb 2021   \n",
      "1   Certified Buyer, Hyderabad     109.0         6.0  Feb 2021   \n",
      "2  Certified Buyer, Dharmapuri      42.0         3.0  Apr 2021   \n",
      "3     Certified Buyer, Chennai      25.0         1.0       NaN   \n",
      "4                          NaN     147.0        24.0  Apr 2016   \n",
      "\n",
      "                                         Review text  Ratings  \n",
      "0  Nice product, good quality, but price is now r...        4  \n",
      "1  They didn't supplied Yonex Mavis 350. Outside ...        1  \n",
      "2  Worst product. Damaged shuttlecocks packed in ...        1  \n",
      "3  Quite O. K. , but nowadays  the quality of the...        3  \n",
      "4  Over pricedJust â?¹620 ..from retailer.I didn'...        1  \n",
      "\n",
      "Column names:\n",
      "Index(['Reviewer Name', 'Review Title', 'Place of Review', 'Up Votes',\n",
      "       'Down Votes', 'Month', 'Review text', 'Ratings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic information about the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8518 entries, 0 to 8517\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Reviewer Name    8508 non-null   object \n",
      " 1   Review Title     8508 non-null   object \n",
      " 2   Place of Review  8468 non-null   object \n",
      " 3   Up Votes         8508 non-null   float64\n",
      " 4   Down Votes       8508 non-null   float64\n",
      " 5   Month            8053 non-null   object \n",
      " 6   Review text      8510 non-null   object \n",
      " 7   Ratings          8518 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 532.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBasic information about the dataset:\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for numerical columns:\n",
      "          Up Votes   Down Votes      Ratings\n",
      "count  8508.000000  8508.000000  8518.000000\n",
      "mean      0.391396     0.121768     4.181028\n",
      "std      11.613909     3.248022     1.262200\n",
      "min       0.000000     0.000000     1.000000\n",
      "25%       0.000000     0.000000     4.000000\n",
      "50%       0.000000     0.000000     5.000000\n",
      "75%       0.000000     0.000000     5.000000\n",
      "max     889.000000   219.000000     5.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSummary statistics for numerical columns:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "Reviewer Name       10\n",
      "Review Title        10\n",
      "Place of Review     50\n",
      "Up Votes            10\n",
      "Down Votes          10\n",
      "Month              465\n",
      "Review text          8\n",
      "Ratings              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the cleaned dataset: (8013, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "print(\"Shape of the cleaned dataset:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows with cleaned and normalized review text:\n",
      "                                         Review text  \\\n",
      "0  Nice product, good quality, but price is now r...   \n",
      "1  They didn't supplied Yonex Mavis 350. Outside ...   \n",
      "2  Worst product. Damaged shuttlecocks packed in ...   \n",
      "3  Quite O. K. , but nowadays  the quality of the...   \n",
      "4  Over pricedJust â?¹620 ..from retailer.I didn'...   \n",
      "\n",
      "                                 Cleaned_Review_Text  \\\n",
      "0  nice product good quality price rising bad sig...   \n",
      "1  didnt supplied yonex mavis outside cover yonex...   \n",
      "2  worst product damaged shuttlecocks packed new ...   \n",
      "3  quite k nowadays quality corks like years back...   \n",
      "4  pricedjust retaileri didnt understand wat adva...   \n",
      "\n",
      "                              Normalized_Review_Text  \n",
      "0  nice product good quality price rising bad sig...  \n",
      "1  didnt supplied yonex mavis outside cover yonex...  \n",
      "2  worst product damaged shuttlecock packed new b...  \n",
      "3  quite k nowadays quality cork like year back u...  \n",
      "4  pricedjust retaileri didnt understand wat adva...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK stopwords data and WordNet lemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\visha\\\\Downloads\\\\reviews_data_dump\\\\reviews_badminton\\\\data.csv\")  # Replace with the actual file name\n",
    "\n",
    "# Define stopwords and initialize WordNet lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    clean_text = ' '.join(filtered_tokens)\n",
    "    return clean_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "data['Cleaned_Review_Text'] = data['Review text'].apply(clean_text)\n",
    "data['Normalized_Review_Text'] = data['Cleaned_Review_Text'].apply(lemmatize_text)\n",
    "\n",
    "print(\"First few rows with cleaned and normalized review text:\")\n",
    "print(data[['Review text', 'Cleaned_Review_Text', 'Normalized_Review_Text']].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Bag-of-Words (BoW)\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_features = count_vectorizer.fit_transform(data['Normalized_Review_Text'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['Normalized_Review_Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BoW features: (8518, 3604)\n",
      "Shape of TF-IDF features: (8518, 3604)\n",
      "Vocabulary size for BoW features: 3604\n",
      "Vocabulary size for TF-IDF features: 3604\n",
      "Feature names for BoW features: ['aa' 'aajao' 'aapke' 'able' 'aboutdelieveryflipkart' 'aboutread'\n",
      " 'absence' 'absolute' 'absolutely' 'abt']\n",
      "Feature names for TF-IDF features: ['aa' 'aajao' 'aapke' 'able' 'aboutdelieveryflipkart' 'aboutread'\n",
      " 'absence' 'absolute' 'absolutely' 'abt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Shape of BoW features:\", bow_features.shape)\n",
    "\n",
    "print(\"Shape of TF-IDF features:\", tfidf_features.shape)\n",
    "\n",
    "print(\"Vocabulary size for BoW features:\", len(count_vectorizer.vocabulary_))\n",
    "\n",
    "print(\"Vocabulary size for TF-IDF features:\", len(tfidf_vectorizer.vocabulary_))\n",
    "\n",
    "print(\"Feature names for BoW features:\", count_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "print(\"Feature names for TF-IDF features:\", tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.54      0.58       145\n",
      "           2       0.50      0.06      0.10        54\n",
      "           3       0.31      0.15      0.20       126\n",
      "           4       0.19      0.01      0.03       353\n",
      "           5       0.67      0.96      0.79      1026\n",
      "\n",
      "    accuracy                           0.64      1704\n",
      "   macro avg       0.46      0.35      0.34      1704\n",
      "weighted avg       0.53      0.64      0.55      1704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Normalized_Review_Text'], data['Ratings'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train SVM with TF-IDF vectors\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions using TF-IDF vectors\n",
    "y_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate TF-IDF + SVM\n",
    "print(\"TF-IDF + SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + SVM Training Score: 0.7091282653360728\n",
      "TF-IDF + SVM Testing Score: 0.6426056338028169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate training and testing scores for TF-IDF + SVM\n",
    "train_score_tfidf = svm_tfidf.score(X_train_tfidf, y_train)\n",
    "test_score_tfidf = svm_tfidf.score(X_test_tfidf, y_test)\n",
    "\n",
    "\n",
    "print(\"TF-IDF + SVM Training Score:\", train_score_tfidf)\n",
    "print(\"TF-IDF + SVM Testing Score:\", test_score_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_tfidf_model.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Save SVM model trained with TF-IDF vectors\n",
    "joblib.dump(svm_tfidf, \"svm_tfidf_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF vectorizer\n",
    "tfidf_vectorizer_loaded = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Load SVM model trained with TF-IDF vectors\n",
    "svm_tfidf_model_loaded = joblib.load(\"svm_tfidf_model.pkl\")\n",
    "\n",
    "# Now you can use tfidf_vectorizer_loaded and svm_tfidf_model_loaded for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vectors:   (0, 113)\t0.1558012405886498\n",
      "  (0, 117)\t0.16859575249393952\n",
      "  (0, 186)\t0.5256140246442065\n",
      "  (0, 269)\t0.12803810932411033\n",
      "  (0, 566)\t0.10612471786959936\n",
      "  (0, 586)\t0.09462607681048593\n",
      "  (0, 593)\t0.12055379964448547\n",
      "  (0, 652)\t0.1690753133942296\n",
      "  (0, 743)\t0.16859575249393952\n",
      "  (0, 1079)\t0.1430067286833601\n",
      "  (0, 1226)\t0.16859575249393952\n",
      "  (0, 1250)\t0.16859575249393952\n",
      "  (0, 1297)\t0.11473785129263805\n",
      "  (0, 1374)\t0.18925215362097186\n",
      "  (0, 1644)\t0.12609330393538679\n",
      "  (0, 1899)\t0.12909317241442242\n",
      "  (0, 1901)\t0.11741770487278065\n",
      "  (0, 1907)\t0.10219448038898418\n",
      "  (0, 2190)\t0.16859575249393952\n",
      "  (0, 2488)\t0.12704010525566753\n",
      "  (0, 2572)\t0.14831693090902492\n",
      "  (0, 2702)\t0.1611114428143146\n",
      "  (0, 2780)\t0.16859575249393952\n",
      "  (0, 2794)\t0.16859575249393952\n",
      "  (0, 2844)\t0.34273678005113345\n",
      "  :\t:\n",
      "  (4921, 1256)\t1.0\n",
      "  (4922, 1080)\t0.4473297360934963\n",
      "  (4922, 2462)\t0.8943691112770628\n",
      "  (4923, 1080)\t0.5357686001917428\n",
      "  (4923, 1660)\t0.8443648542239194\n",
      "  (4924, 969)\t0.6893309694880039\n",
      "  (4924, 1651)\t0.4639708223255114\n",
      "  (4924, 2176)\t0.5563756739248379\n",
      "  (4925, 1660)\t1.0\n",
      "  (4926, 857)\t1.0\n",
      "  (4927, 236)\t0.2777406216549741\n",
      "  (4927, 586)\t0.3500041654554449\n",
      "  (4927, 646)\t0.37089839497513577\n",
      "  (4927, 1157)\t0.4157399074696554\n",
      "  (4927, 1978)\t0.31791474663895003\n",
      "  (4927, 2188)\t0.6236041653629559\n",
      "  (4928, 1651)\t0.3565777137385589\n",
      "  (4928, 2010)\t0.3265254368750965\n",
      "  (4928, 2731)\t0.8753476298811288\n",
      "  (4929, 1053)\t0.5461836307998909\n",
      "  (4929, 1757)\t0.7626850970396777\n",
      "  (4929, 2038)\t0.3464027774135563\n",
      "  (4930, 171)\t0.8301822198337184\n",
      "  (4930, 1098)\t0.35676753852072357\n",
      "  (4930, 2445)\t0.42838581364212297\n",
      "Coefficients:   (0, 549)\t-0.354703090255712\n",
      "  (0, 2095)\t-0.354703090255712\n",
      "  (0, 2481)\t-0.2885002201485192\n",
      "  (0, 434)\t-0.5067296921247666\n",
      "  (0, 1760)\t-0.5302694350100297\n",
      "  (0, 2808)\t-0.6126857799514577\n",
      "  (0, 764)\t-0.6509499545476738\n",
      "  (0, 1157)\t-0.43397060010578037\n",
      "  (0, 2955)\t-1.0\n",
      "  (0, 2057)\t-0.9656468389644896\n",
      "  (0, 1256)\t-0.962562208844155\n",
      "  (0, 495)\t-0.6082421452734008\n",
      "  (0, 627)\t-0.3280108251248665\n",
      "  (0, 1141)\t-0.35494731414939446\n",
      "  (0, 1804)\t-0.5482756941516211\n",
      "  (0, 2240)\t-0.3193392125302459\n",
      "  (0, 748)\t-0.5673298824113386\n",
      "  (0, 2892)\t-0.5673298824113386\n",
      "  (0, 712)\t-0.3515641257719919\n",
      "  (0, 1160)\t-1.1262049769251372\n",
      "  (0, 1949)\t-0.8839032603728474\n",
      "  (0, 2080)\t-0.6125503899663203\n",
      "  (0, 2186)\t-0.530406905887138\n",
      "  (0, 909)\t-0.5549283647833028\n",
      "  (0, 2903)\t-0.6822691703938958\n",
      "  :\t:\n",
      "  (9, 522)\t0.27373677038211164\n",
      "  (9, 1001)\t0.27373677038211164\n",
      "  (9, 1403)\t0.27373677038211164\n",
      "  (9, 1668)\t0.27373677038211164\n",
      "  (9, 2830)\t0.026307858226946562\n",
      "  (9, 1011)\t0.427499232535692\n",
      "  (9, 2468)\t0.427499232535692\n",
      "  (9, 782)\t0.31177293417228047\n",
      "  (9, 816)\t0.28811289256976297\n",
      "  (9, 1993)\t0.21563486013626543\n",
      "  (9, 2605)\t0.31177293417228047\n",
      "  (9, 220)\t0.5969780784988725\n",
      "  (9, 880)\t0.5969780784988725\n",
      "  (9, 2134)\t0.962562208844155\n",
      "  (9, 1593)\t0.4471040944278928\n",
      "  (9, 2138)\t0.4471040944278928\n",
      "  (9, 2631)\t0.4471040944278928\n",
      "  (9, 1806)\t0.3535432678199234\n",
      "  (9, 1925)\t0.6999573042236191\n",
      "  (9, 222)\t0.21362638369754228\n",
      "  (9, 1516)\t-0.20081644033920842\n",
      "  (9, 1690)\t-0.22955648243604837\n",
      "  (9, 575)\t0.9558874640134637\n",
      "  (9, 641)\t0.7963775544217679\n",
      "  (9, 2953)\t1.0\n",
      "Intercepts: [ 0.60328555  0.08307882 -0.30652561 -0.65141404 -0.52233633 -0.69305887\n",
      " -0.83239631 -0.47567868 -0.79106118 -0.70675112]\n",
      "Kernel: linear\n",
      "Gamma: scale\n",
      "Degree: 3\n"
     ]
    }
   ],
   "source": [
    "# Inspect support vectors\n",
    "support_vectors = svm_tfidf.support_vectors_\n",
    "\n",
    "# Inspect coefficients\n",
    "coefficients = svm_tfidf.coef_\n",
    "\n",
    "# Inspect intercepts\n",
    "intercepts = svm_tfidf.intercept_\n",
    "\n",
    "# Inspect kernel parameters\n",
    "kernel = svm_tfidf.kernel\n",
    "gamma = svm_tfidf.gamma\n",
    "degree = svm_tfidf.degree\n",
    "\n",
    "# Print information about the model\n",
    "print(\"Support vectors:\", support_vectors)\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercepts:\", intercepts)\n",
    "print(\"Kernel:\", kernel)\n",
    "print(\"Gamma:\", gamma)\n",
    "print(\"Degree:\", degree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
